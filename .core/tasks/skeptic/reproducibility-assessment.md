# reproducibility-assessment

## ⚠️ CRITICAL EXECUTION NOTICE ⚠️

**THIS IS AN EXECUTABLE TASK - NOT REFERENCE MATERIAL**

Apply systematic reproducibility assessment methodology to evaluate research replicability, methodological adequacy, and result reliability through comprehensive critical validation.

**CORE PRINCIPLE**: Reproducibility assessment enables scientific reliability - systematic replication evaluation accelerates trustworthy conclusions through rigorous validation and verification.

## Method Description

**Physics Reproducibility Assessment System (PRAS)**: Comprehensive reproducibility evaluation framework combining methodological assessment, documentation analysis, replication testing, and systematic reliability validation for rigorous physics research critical evaluation.

**Key Innovation**: Integrates multiple reproducibility evaluation methodologies while maintaining assessment efficiency and enabling systematic reliability quality assurance across all physics research domains.

## Reproducibility Assessment Process

### Phase 1: Reproducibility Framework Analysis
1. **Web Search Standards**: Research reproducibility assessment standards in target physics domain
2. **Methodology Documentation Review**: Systematic evaluation of method documentation completeness and clarity
3. **Implementation Assessment**: Analysis of implementation adequacy for independent reproduction
4. **Resource Requirement Analysis**: Evaluation of resource accessibility for reproduction attempts
5. **Quality Control Integration**: Assessment of quality control measures supporting reproducibility

### Phase 2: Systematic Reproducibility Evaluation

**Step 1: Documentation Completeness Assessment**
- Evaluate method documentation for sufficient detail and clarity
- Assess parameter specification completeness and precision
- Review equipment and software documentation adequacy
- Evaluate environmental condition documentation and control
- Assess data documentation and preservation adequacy

**Step 2: Implementation Feasibility Analysis**
- Evaluate implementation clarity and feasibility for independent researchers
- Assess resource requirements and accessibility for reproduction attempts
- Review technical complexity and expertise requirements
- Evaluate time and cost requirements for independent reproduction
- Assess institutional support and infrastructure requirements

**Step 3: Replication Testing and Validation**
- Design systematic replication protocols and testing procedures
- Execute replication attempts using documented procedures
- Assess replication success rates and failure modes
- Evaluate replication quality and consistency with original results
- Document replication challenges and improvement opportunities

### Phase 3: Reproducibility Quality Assessment and Improvement

**Step 1: Reproducibility Quality Scoring**
- Calculate systematic reproducibility scores using established criteria
- Assess reproducibility across multiple dimensions (documentation, feasibility, success)
- Evaluate reproducibility uncertainty and confidence assessment
- Compare reproducibility achievement against established standards
- Document reproducibility assessment results and recommendations

**Step 2: Reproducibility Improvement Strategy Development**
- Identify systematic reproducibility weaknesses and improvement opportunities
- Develop reproducibility enhancement strategies and implementation plans
- Create reproducibility monitoring and quality control procedures
- Plan reproducibility validation and verification protocols
- Establish reproducibility improvement and learning frameworks

## Interactive Flow

### Step 1: Reproducibility Assessment Scope Definition
**Question**: "What physics research requires reproducibility assessment and what are the specific replication concerns or validation requirements?"

*Wait for user response, then systematically define reproducibility evaluation scope*

### Step 2: Comprehensive Reproducibility Analysis
**Action**: Execute systematic reproducibility evaluation across all relevant dimensions
- Evaluate method documentation completeness and clarity for independent reproduction
- Assess implementation feasibility and resource requirements for replication attempts
- Analyze quality control measures and validation procedures supporting reproducibility
- Review data documentation and preservation adequacy for independent analysis
- Evaluate institutional support and infrastructure requirements for reproduction

### Step 3: Replication Testing and Validation
**Action**: Implement systematic replication testing and assessment
- Design replication protocols and testing procedures based on available documentation
- Execute replication attempts using systematic testing and validation procedures
- Assess replication success rates and analyze failure modes and challenges
- Evaluate replication quality and consistency with original research results
- Document replication testing results and improvement recommendations

### Step 4: Reproducibility Quality Assessment and Improvement
**Action**: Assess reproducibility quality and develop improvement strategies
- Calculate systematic reproducibility scores using established assessment criteria
- Evaluate reproducibility achievement against established standards and best practices
- Develop reproducibility improvement strategies for identified weaknesses
- Plan reproducibility monitoring and quality control for ongoing assessment
- Prepare reproducibility assessment communication with transparent reporting

## Output Format

### Primary Output: reproducibility-assessment-report.md
```yaml
reproducibility_assessment:
  assessment_id: "RA_{timestamp}_{sequence}"
  assessment_date: "{YYYY-MM-DD}"
  physics_domain: "{specific physics domain or research area}"
  research_context: "{comprehensive description of research requiring reproducibility assessment}"
  assessment_scope: "{systematic description of reproducibility evaluation objectives}"
  
documentation_assessment:
  method_documentation:
    completeness_score: "{quantitative assessment of documentation completeness}"
    clarity_assessment: ["{evaluation of documentation clarity and understandability}"]
    parameter_specification: ["{assessment of parameter documentation precision}"]
    missing_information: ["{systematic identification of missing documentation}"]
    
  implementation_documentation:
    equipment_documentation: ["{assessment of equipment specification adequacy}"]
    software_documentation: ["{evaluation of software environment documentation}"]
    environmental_documentation: ["{assessment of environmental condition documentation}"]
    quality_control_documentation: ["{evaluation of quality control procedure documentation}"]
    
replication_feasibility:
  resource_requirements:
    equipment_accessibility: ["{assessment of equipment accessibility for independent reproduction}"]
    expertise_requirements: ["{evaluation of required expertise and training}"]
    time_requirements: ["{realistic time estimates for independent reproduction}"]
    cost_requirements: ["{estimated costs for independent reproduction attempts}"]
    
  technical_feasibility:
    complexity_assessment: ["{evaluation of implementation complexity and difficulty}"]
    dependency_analysis: ["{assessment of external dependencies and availability}"]
    infrastructure_requirements: ["{institutional infrastructure needs for reproduction}"]
    support_availability: ["{availability of support and assistance for reproduction}"]
    
replication_testing:
  replication_attempts:
    - attempt_id: "REPLICATION_001"
      attempt_description: "{systematic description of replication attempt}"]
      replication_success: "{assessment of replication success or failure}"
      result_consistency: "{comparison of replication results with original}"]
      challenges_encountered: ["{specific challenges and difficulties in replication}"]
      recommendations: ["{recommendations for improving replication success}"]
      
  replication_quality:
    success_rate: "{percentage of successful replication attempts}"
    result_consistency: "{quantitative assessment of result consistency}"
    challenge_patterns: ["{systematic patterns in replication challenges}"]
    improvement_opportunities: ["{specific opportunities for reproducibility enhancement}"]
    
reproducibility_scoring:
  overall_score: "{comprehensive reproducibility score (0-100)}"
  component_scores:
    documentation: "{documentation quality score}"
    feasibility: "{replication feasibility score}"
    success_rate: "{replication success score}"
    quality: "{replication quality score}"
    
  scoring_rationale: ["{systematic justification for reproducibility scores}"]
  uncertainty_assessment: ["{confidence and uncertainty in reproducibility assessment}"]
  
improvement_strategy:
  priority_improvements: ["{highest priority reproducibility improvements}"]
  implementation_plan: ["{systematic plan for reproducibility enhancement}"]
  monitoring_framework: ["{ongoing reproducibility monitoring and assessment}"]
  validation_procedures: ["{validation of reproducibility improvements}"]

next_action: "Implement reproducibility improvements with systematic monitoring and validation"
```

## Integration Points

**With Other Tasks**:
- Builds on `bias-detection` with bias-free reproducibility assessment
- Coordinates with `statistical-analysis` for statistical reproducibility validation
- Supports `evidence-evaluation` with reproducibility quality evidence
- Integrates with `comprehensive-validation` for overall research validation

**With Templates**:
- Generates reproducibility-assessment-report.md using reproducibility template
- Provides data for statistical-review-tmpl.yaml with reproducibility evidence
- Supports critique-analysis-tmpl.yaml with reproducibility validation

**With Agent Commands**:
- Triggered by reproducibility validation needs in Skeptic agent critical evaluation
- Results stored in agent state for reproducibility tracking and monitoring
- Provides reproducibility evidence for constructive tension with other agents

## Validation Criteria

**Reproducibility Assessment Effectiveness**:
- Documentation assessment comprehensive with systematic evaluation of completeness and clarity
- Replication feasibility analysis thorough with resource and technical requirement assessment
- Replication testing systematic with appropriate success criteria and quality evaluation
- Improvement strategies practical with clear implementation and monitoring frameworks

**Korean Philosophy Integration**:
- Embodies '이것이 정말 맞는가? 다른 설명은?' (is this really correct? what other explanations?)
- Maintains systematic reproducibility evaluation as foundation for research reliability
- Supports constructive tension with implementation through reproducibility feedback
- Enables scientific trustworthiness through systematic reproducibility validation and improvement

## Notes for Skeptic Agent

**Reproducibility Assessment Priorities**:
- Apply comprehensive reproducibility assessment as foundation for research validation
- Focus reproducibility evaluation on areas with highest impact on research reliability
- Implement reproducibility assessment with systematic quality control and improvement
- Maintain reproducibility assessment as ongoing validation process throughout research

**Korean Philosophy Application**:
- Apply systematic reproducibility rigor (체계적 재현성 엄격성) to all critical evaluation
- Enable rigorous validation through comprehensive reproducibility assessment
- Support constructive tension with other agents through reproducibility feedback and improvement
- Promote scientific reliability through systematic reproducibility validation and enhancement